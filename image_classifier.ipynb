{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9aa7ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as loader\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e0ab87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xvzf ./101_ObjectCategories.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943dce2",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0749427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(path):\n",
    "    files = os.listdir(path)\n",
    "    return np.asarray(files)\n",
    "\n",
    "\n",
    "def split_data(prev, curr, classes):\n",
    "    for name in classes:\n",
    "        curr_dir = os.path.join(os.getcwd(), f\"{prev}/{name}\")\n",
    "        \n",
    "        files = ls(curr_dir)\n",
    "        total_files = np.size(files, 0)\n",
    "        print(f'Total files : {total_files}')\n",
    "        \n",
    "        # Training data 75%\n",
    "        train_size = math.ceil(total_files * 3/4)\n",
    "        print(f'Total files : {train_size}')\n",
    "        \n",
    "        # Validation data 12.5\n",
    "        validation_size = train_size + math.ceil(total_files * 1/8) #used for indexing so\n",
    "        print(f'Total files : {validation_size - train_size}')\n",
    "        \n",
    "        # Test data 12.5%\n",
    "        test_size = validation_size + math.ceil(total_files * 1/8)\n",
    "        print(f'Total files : {test_size - validation_size}')\n",
    "        \n",
    "        train = files[0:train_size]\n",
    "        validation = files[train_size:validation_size]\n",
    "        test = files[validation_size:]\n",
    "        \n",
    "        # Creating directories for all data splits and moving them accordingly\n",
    "        mv(train, curr_dir, f'train/{name}')\n",
    "        mv(validation, curr_dir, f'validation/{name}')\n",
    "        mv(test, curr_dir, f'test/{name}')\n",
    "        \n",
    "        \n",
    "def mv(files, prev, curr):\n",
    "    curr = os.path.join(os.getcwd(), curr);\n",
    "    if not os.path.exists(curr):\n",
    "        os.makedirs(curr)\n",
    "        \n",
    "    for file in np.nditer(files):\n",
    "        prev_path = os.path.join(os.getcwd(), f'{prev}/{file}')\n",
    "        curr_path = os.path.join(os.getcwd(), f'{curr}/{file}')\n",
    "        \n",
    "        shutil.move(prev_path, curr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f780e67",
   "metadata": {},
   "source": [
    "### Input Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c711dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_transform():\n",
    "    image_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "                  transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "                  transforms.RandomRotation(degrees=15),\n",
    "                  transforms.RandomHorizontalFlip(),\n",
    "                  transforms.CenterCrop(size=224),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'validation': transforms.Compose([\n",
    "                 transforms.Resize(size=256),\n",
    "                 transforms.CenterCrop(size=224),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])              \n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "                 transforms.Resize(size=256),\n",
    "                 transforms.CenterCrop(size=224),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])              \n",
    "        ])\n",
    "    }\n",
    "    return image_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f6743",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "75365914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = {\n",
    "        'train': datasets.ImageFolder(root='./train', transform=image_transforms['train']),\n",
    "        'validation': datasets.ImageFolder(root='./validation', transform=image_transforms['validation']),\n",
    "        'test': datasets.ImageFolder(root='./test', transform=image_transforms['test'])\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101216aa",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec310933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def def_model():\n",
    "    model = models.resnet50(pretrained=True)\n",
    "     \n",
    "    for param in model.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    fc_inputs = model.fc.in_features\n",
    "\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(fc_inputs, 2048),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(2048, 10),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "    \n",
    "    if(torch.cuda.is_available()):\n",
    "        model = model.to('cuda:0')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaec469",
   "metadata": {},
   "source": [
    "### Optimization of loss/cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "611b4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_loss(model):\n",
    "    loss_func = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    return loss_func, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f873a22",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a2c2aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  start = time.time()\n",
    "  history = []\n",
    "  best_acc = 0.0\n",
    "  for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(f'Epoch : {epoch+1}/{epochs}')\n",
    "    model.train()\n",
    "    train_loss = 0.0 \n",
    "    train_acc = 0.0\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    valid_acc  = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_data):\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      # Clean existing gradients\n",
    "      optimizer.zero_grad()\n",
    "      # Forward pass - compute outputs on input data using the model\n",
    "      outputs = model(inputs)\n",
    "      # Compute loss\n",
    "      loss = loss_criterion(outputs, labels)\n",
    "      # Backpropagate the gradients\n",
    "      loss.backward()\n",
    "      # Update the parameters\n",
    "      optimizer.step()\n",
    "      # Compute the total loss for the batch and it to train_loss\n",
    "      train_loss += loss.item() * inputs.size(0)\n",
    "      # Compute the accuracy\n",
    "      ret, predictions = torch.max(outputs.data,1)\n",
    "      correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "      # Convert correct_counts to float and then compute the mean\n",
    "      acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "      # Compute total accuracy in the whole batch and add to train_acc\n",
    "      train_acc += acc.item() * inputs.size(0)\n",
    "      print(f'Batch number: {i}, Training: Loss: {loss.item()}, Accuracy: {acc.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "      model.eval()\n",
    "      for j, (inputs, labels) in enumerate(validation_data): \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_criterion(outputs, labels)\n",
    "        # Compute the total loss for  the batch and add it to valid_loss\n",
    "        valid_loss += loss.item() * inputs.size(0)\n",
    "        # Calculate validation accuracy\n",
    "\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_prediction_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "        # Convert correct_prediction_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_prediction_counts.type(torch.FloatTensor))\n",
    "\n",
    "        # Compute total accuracy in the whole batch and add to valid_acc\n",
    "\n",
    "        valid_acc +=acc.item() * inputs.size(0)\n",
    "\n",
    "      avg_train_loss = train_loss/train_data_size\n",
    "      avg_train_acc = train_acc/train_data_size\n",
    "\n",
    "      avg_valid_loss = valid_loss/validation_data_size\n",
    "      avg_valid_acc = valid_acc/validation_data_size\n",
    "\n",
    "      history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "\n",
    "      epoch_end = time.time()\n",
    "\n",
    "      print(f'Epoch : {epoch}, Training: Loss: f{avg_train_loss}, Accuracy: {avg_train_acc*100}%, \\n\\t\\tValidation : Loss : {avg_valid_loss}, Accuracy: {avg_valid_acc*100}%, Time: {epoch_end-epoch_start}s')\n",
    "\n",
    "      # Save if the model has best accuracy till now\n",
    "\n",
    "      torch.save(model.state_dict(), f'model_{epoch}.pth')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf17a5a",
   "metadata": {},
   "source": [
    "### Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3a32fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeModelAccuracy(model, loss_criterion):\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  test_acc = 0.0\n",
    "  test_loss = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    for i, (inputs, labels) in enumerate(test_data):\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # Compute loss\n",
    "      loss = loss_criterion(outputs, labels)\n",
    "\n",
    "      # Compute the toal loss item \n",
    "      test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "      ret, predictions = torch.max(outputs.data, 1)\n",
    "      correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "      acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "      test_acc +=acc.item() * inputs.size(0)\n",
    "\n",
    "      print(f'Test Batch number: {i}, Test: Loss: {loss.item()}, Accuracy: {acc.item()}')\n",
    "\n",
    "      # Find average test loss and test accuracy\n",
    "      avg_test_loss = test_loss/test_data_size\n",
    "      avg_test_acc = test_acc/test_data_size\n",
    "\n",
    "      print(f'Test accuracy: {avg_test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c4c97e",
   "metadata": {},
   "source": [
    "### Model prediction and image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0d05fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(model, url):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform = image_transforms['test']\n",
    "\n",
    "    test_image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    plt.imshow(test_image)\n",
    "\n",
    "    test_image_tensor = transform(test_image)\n",
    "    test_image_tensor = test_image_tensor.view(1, 3, 224, 224).to(device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(test_image_tensor)\n",
    "        ps = torch.exp(out)\n",
    "\n",
    "        topk, topclass = ps.topk(3, dim=1)\n",
    "        for i in range(3):\n",
    "            print(f\"Prediction {i+1} : {index_to_class[topclass.cpu().numpy()[0][i]]}, Score: {topk.cpu().numpy()[0][i] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8d4f0",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "72e7458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # split dat\n",
    "#     classes = ['Leopards', 'airplanes', 'butterfly', 'camera', 'elephant', 'lamp', 'watch', 'umbrella', 'rhino'];\n",
    "#     split_data('101_ObjectCategories', './', classes)\n",
    "    \n",
    "#     # transform data\n",
    "#     input_transform()\n",
    "    \n",
    "#     # created dataset\n",
    "#     batch_size = 10\n",
    "#     data = load_data()\n",
    "    \n",
    "#     # create a data loader instance with each dataset with a batch size of 10 and shuffling\n",
    "#     train_data = loader.DataLoader(data['train'], batch_size=batch_size, shuffle=True)\n",
    "#     validation_data = loader.DataLoader(data['validation'], batch_size=batch_size, shuffle=True)\n",
    "#     test_data = loader.DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "#     #data size\n",
    "#     train_data_size = len(data['train'])\n",
    "#     validation_data_size = len(data['validation'])\n",
    "#     test_data_size =  len(data['test'])\n",
    "    \n",
    "#     # get model and change r50 model output to classify\n",
    "#     model = def_model()\n",
    "    \n",
    "#     # select optimiser and define loss func\n",
    "#     loss_func, optimizer = opt_loss(model)\n",
    "    \n",
    "#     # fitting / backpropagation\n",
    "#     num_epochs = 30\n",
    "#     trained_model, history = train_and_validate(model, loss_func, optimizer, num_epochs)\n",
    "    \n",
    "#     # accuracy on testdata\n",
    "#     model = models.resnet50(pretrained=False)\n",
    "#     fc_inputs = model.fc.in_features\n",
    "\n",
    "#     model.fc = nn.Sequential(\n",
    "#         nn.Linear(fc_inputs, 2048),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.Linear(2048, 10),\n",
    "#         nn.Dropout(0.4),\n",
    "#         nn.LogSoftmax(dim=1))\n",
    "\n",
    "#     if(torch.cuda.is_available()):\n",
    "#         model = model.to(\"cuda\")\n",
    "#     model.load_state_dict(torch.load('model_0.pth'))\n",
    "\n",
    "#     computeModelAccuracy(model, loss_func)\n",
    "    \n",
    "#     # testing \n",
    "#     index_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
    "#     print (index_to_class)\n",
    "\n",
    "#     makePrediction(model, 'https://cdn.britannica.com/30/136130-050-3370E37A/Leopard.jpg')\n",
    "# #     model = torch.load('_model_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d94b95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f46203aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files : 200\n",
      "Total files : 150\n",
      "Total files : 25\n",
      "Total files : 25\n",
      "Total files : 800\n",
      "Total files : 600\n",
      "Total files : 100\n",
      "Total files : 100\n",
      "Total files : 91\n",
      "Total files : 69\n",
      "Total files : 12\n",
      "Total files : 12\n",
      "Total files : 50\n",
      "Total files : 38\n",
      "Total files : 7\n",
      "Total files : 7\n",
      "Total files : 64\n",
      "Total files : 48\n",
      "Total files : 8\n",
      "Total files : 8\n",
      "Total files : 61\n",
      "Total files : 46\n",
      "Total files : 8\n",
      "Total files : 8\n",
      "Total files : 239\n",
      "Total files : 180\n",
      "Total files : 30\n",
      "Total files : 30\n",
      "Total files : 75\n",
      "Total files : 57\n",
      "Total files : 10\n",
      "Total files : 10\n",
      "Total files : 59\n",
      "Total files : 45\n",
      "Total files : 8\n",
      "Total files : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91628\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91628\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/30\n",
      "Batch number: 0, Training: Loss: 2.317222833633423, Accuracy: 0.0\n",
      "Batch number: 1, Training: Loss: 6.524583339691162, Accuracy: 0.20000000298023224\n",
      "Batch number: 2, Training: Loss: 3.7636077404022217, Accuracy: 0.5\n",
      "Batch number: 3, Training: Loss: 4.386628150939941, Accuracy: 0.30000001192092896\n",
      "Batch number: 4, Training: Loss: 1.8683111667633057, Accuracy: 0.699999988079071\n",
      "Batch number: 5, Training: Loss: 2.348623752593994, Accuracy: 0.10000000149011612\n",
      "Batch number: 6, Training: Loss: 3.3893685340881348, Accuracy: 0.10000000149011612\n",
      "Batch number: 7, Training: Loss: 2.557569980621338, Accuracy: 0.20000000298023224\n",
      "Batch number: 8, Training: Loss: 2.283421277999878, Accuracy: 0.30000001192092896\n",
      "Batch number: 9, Training: Loss: 1.911400556564331, Accuracy: 0.30000001192092896\n",
      "Batch number: 10, Training: Loss: 1.563279390335083, Accuracy: 0.5\n",
      "Batch number: 11, Training: Loss: 1.4541476964950562, Accuracy: 0.6000000238418579\n",
      "Batch number: 12, Training: Loss: 1.8728163242340088, Accuracy: 0.699999988079071\n",
      "Batch number: 13, Training: Loss: 4.452160358428955, Accuracy: 0.4000000059604645\n",
      "Batch number: 14, Training: Loss: 3.063201665878296, Accuracy: 0.30000001192092896\n",
      "Batch number: 15, Training: Loss: 2.0960936546325684, Accuracy: 0.30000001192092896\n",
      "Batch number: 16, Training: Loss: 1.4296849966049194, Accuracy: 0.699999988079071\n",
      "Batch number: 17, Training: Loss: 0.9188860058784485, Accuracy: 0.6000000238418579\n",
      "Batch number: 18, Training: Loss: 2.274689197540283, Accuracy: 0.30000001192092896\n",
      "Batch number: 19, Training: Loss: 1.4820642471313477, Accuracy: 0.4000000059604645\n",
      "Batch number: 20, Training: Loss: 1.3661763668060303, Accuracy: 0.5\n",
      "Batch number: 21, Training: Loss: 1.9332568645477295, Accuracy: 0.30000001192092896\n",
      "Batch number: 22, Training: Loss: 1.143213152885437, Accuracy: 0.699999988079071\n",
      "Batch number: 23, Training: Loss: 1.5569757223129272, Accuracy: 0.30000001192092896\n",
      "Batch number: 24, Training: Loss: 1.4066781997680664, Accuracy: 0.6000000238418579\n",
      "Batch number: 25, Training: Loss: 1.249869704246521, Accuracy: 0.6000000238418579\n",
      "Batch number: 26, Training: Loss: 2.2091004848480225, Accuracy: 0.10000000149011612\n",
      "Batch number: 27, Training: Loss: 0.9681937098503113, Accuracy: 0.6000000238418579\n",
      "Batch number: 28, Training: Loss: 1.5374248027801514, Accuracy: 0.4000000059604645\n",
      "Batch number: 29, Training: Loss: 0.9192129373550415, Accuracy: 0.699999988079071\n",
      "Batch number: 30, Training: Loss: 1.3106718063354492, Accuracy: 0.6000000238418579\n",
      "Batch number: 31, Training: Loss: 0.8873526453971863, Accuracy: 0.8999999761581421\n",
      "Batch number: 32, Training: Loss: 1.140214443206787, Accuracy: 0.699999988079071\n",
      "Batch number: 33, Training: Loss: 1.7964789867401123, Accuracy: 0.4000000059604645\n",
      "Batch number: 34, Training: Loss: 1.2594341039657593, Accuracy: 0.5\n",
      "Batch number: 35, Training: Loss: 0.9867426156997681, Accuracy: 0.699999988079071\n",
      "Batch number: 36, Training: Loss: 1.0060430765151978, Accuracy: 0.699999988079071\n",
      "Batch number: 37, Training: Loss: 0.8557760119438171, Accuracy: 0.8999999761581421\n",
      "Batch number: 38, Training: Loss: 1.474729299545288, Accuracy: 0.6000000238418579\n",
      "Batch number: 39, Training: Loss: 1.5123355388641357, Accuracy: 0.4000000059604645\n",
      "Batch number: 40, Training: Loss: 2.0073893070220947, Accuracy: 0.20000000298023224\n",
      "Batch number: 41, Training: Loss: 1.5199506282806396, Accuracy: 0.5\n",
      "Batch number: 42, Training: Loss: 1.0230677127838135, Accuracy: 0.6000000238418579\n",
      "Batch number: 43, Training: Loss: 1.0375354290008545, Accuracy: 0.6000000238418579\n",
      "Batch number: 44, Training: Loss: 1.3724894523620605, Accuracy: 0.4000000059604645\n",
      "Batch number: 45, Training: Loss: 0.9075952768325806, Accuracy: 0.699999988079071\n",
      "Batch number: 46, Training: Loss: 0.7162520289421082, Accuracy: 0.699999988079071\n",
      "Batch number: 47, Training: Loss: 0.8310526013374329, Accuracy: 0.699999988079071\n",
      "Batch number: 48, Training: Loss: 1.102427363395691, Accuracy: 0.5\n",
      "Batch number: 49, Training: Loss: 1.2260528802871704, Accuracy: 0.5\n",
      "Batch number: 50, Training: Loss: 0.9087580442428589, Accuracy: 0.699999988079071\n",
      "Batch number: 51, Training: Loss: 1.7829430103302002, Accuracy: 0.4000000059604645\n",
      "Batch number: 52, Training: Loss: 1.2667593955993652, Accuracy: 0.699999988079071\n",
      "Batch number: 53, Training: Loss: 1.110748052597046, Accuracy: 0.699999988079071\n",
      "Batch number: 54, Training: Loss: 1.2286354303359985, Accuracy: 0.6000000238418579\n",
      "Batch number: 55, Training: Loss: 1.2006862163543701, Accuracy: 0.30000001192092896\n",
      "Batch number: 56, Training: Loss: 0.5827282667160034, Accuracy: 0.8999999761581421\n",
      "Batch number: 57, Training: Loss: 0.7581644058227539, Accuracy: 0.6000000238418579\n",
      "Batch number: 58, Training: Loss: 1.3831651210784912, Accuracy: 0.6000000238418579\n",
      "Batch number: 59, Training: Loss: 0.4670008718967438, Accuracy: 0.800000011920929\n",
      "Batch number: 60, Training: Loss: 1.2781627178192139, Accuracy: 0.4000000059604645\n",
      "Batch number: 61, Training: Loss: 0.9081883430480957, Accuracy: 0.8999999761581421\n",
      "Batch number: 62, Training: Loss: 0.9871387481689453, Accuracy: 0.699999988079071\n",
      "Batch number: 63, Training: Loss: 0.950624942779541, Accuracy: 0.5\n",
      "Batch number: 64, Training: Loss: 0.6452386379241943, Accuracy: 1.0\n",
      "Batch number: 65, Training: Loss: 0.7157124280929565, Accuracy: 0.800000011920929\n",
      "Batch number: 66, Training: Loss: 1.5401182174682617, Accuracy: 0.6000000238418579\n",
      "Batch number: 67, Training: Loss: 0.7772418260574341, Accuracy: 0.6000000238418579\n",
      "Batch number: 68, Training: Loss: 0.9308916330337524, Accuracy: 0.699999988079071\n",
      "Batch number: 69, Training: Loss: 1.0394084453582764, Accuracy: 0.6000000238418579\n",
      "Batch number: 70, Training: Loss: 0.9646168947219849, Accuracy: 0.800000011920929\n",
      "Batch number: 71, Training: Loss: 1.255581259727478, Accuracy: 0.699999988079071\n",
      "Batch number: 72, Training: Loss: 1.0153911113739014, Accuracy: 0.6000000238418579\n",
      "Batch number: 73, Training: Loss: 0.5071045160293579, Accuracy: 0.800000011920929\n",
      "Batch number: 74, Training: Loss: 1.034125804901123, Accuracy: 0.6000000238418579\n",
      "Batch number: 75, Training: Loss: 1.1244710683822632, Accuracy: 0.6000000238418579\n",
      "Batch number: 76, Training: Loss: 0.8352422714233398, Accuracy: 0.6000000238418579\n",
      "Batch number: 77, Training: Loss: 0.3048374056816101, Accuracy: 0.8999999761581421\n",
      "Batch number: 78, Training: Loss: 0.5599910020828247, Accuracy: 0.800000011920929\n",
      "Batch number: 79, Training: Loss: 1.6772228479385376, Accuracy: 0.4000000059604645\n",
      "Batch number: 80, Training: Loss: 1.225005030632019, Accuracy: 0.6000000238418579\n",
      "Batch number: 81, Training: Loss: 0.8818410038948059, Accuracy: 0.699999988079071\n",
      "Batch number: 82, Training: Loss: 1.0357176065444946, Accuracy: 0.800000011920929\n",
      "Batch number: 83, Training: Loss: 0.6679370999336243, Accuracy: 0.8999999761581421\n",
      "Batch number: 84, Training: Loss: 1.2441112995147705, Accuracy: 0.4000000059604645\n",
      "Batch number: 85, Training: Loss: 1.5393285751342773, Accuracy: 0.5\n",
      "Batch number: 86, Training: Loss: 0.5849742889404297, Accuracy: 0.800000011920929\n",
      "Batch number: 87, Training: Loss: 1.4228708744049072, Accuracy: 0.5\n",
      "Batch number: 88, Training: Loss: 0.6763208508491516, Accuracy: 0.699999988079071\n",
      "Batch number: 89, Training: Loss: 0.8819316029548645, Accuracy: 0.800000011920929\n",
      "Batch number: 90, Training: Loss: 0.5795615315437317, Accuracy: 0.8999999761581421\n",
      "Batch number: 91, Training: Loss: 0.616596519947052, Accuracy: 0.8999999761581421\n",
      "Batch number: 92, Training: Loss: 0.46247607469558716, Accuracy: 0.800000011920929\n",
      "Batch number: 93, Training: Loss: 0.7500728368759155, Accuracy: 0.699999988079071\n",
      "Batch number: 94, Training: Loss: 0.5368010401725769, Accuracy: 0.8999999761581421\n",
      "Batch number: 95, Training: Loss: 1.1167876720428467, Accuracy: 0.4000000059604645\n",
      "Batch number: 96, Training: Loss: 0.8220174908638, Accuracy: 0.800000011920929\n",
      "Batch number: 97, Training: Loss: 1.0982095003128052, Accuracy: 0.699999988079071\n",
      "Batch number: 98, Training: Loss: 1.009294867515564, Accuracy: 0.6000000238418579\n",
      "Batch number: 99, Training: Loss: 0.6136287450790405, Accuracy: 0.699999988079071\n",
      "Batch number: 100, Training: Loss: 0.6429224610328674, Accuracy: 1.0\n",
      "Batch number: 101, Training: Loss: 1.0134804248809814, Accuracy: 0.6000000238418579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 102, Training: Loss: 1.4027235507965088, Accuracy: 0.699999988079071\n",
      "Batch number: 103, Training: Loss: 1.1437294483184814, Accuracy: 0.800000011920929\n",
      "Batch number: 104, Training: Loss: 1.0934722423553467, Accuracy: 0.800000011920929\n",
      "Batch number: 105, Training: Loss: 0.7348465919494629, Accuracy: 0.699999988079071\n",
      "Batch number: 106, Training: Loss: 0.681165337562561, Accuracy: 0.800000011920929\n",
      "Batch number: 107, Training: Loss: 0.726866602897644, Accuracy: 0.699999988079071\n",
      "Batch number: 108, Training: Loss: 0.5208412408828735, Accuracy: 0.800000011920929\n",
      "Batch number: 109, Training: Loss: 0.6325676441192627, Accuracy: 0.800000011920929\n",
      "Batch number: 110, Training: Loss: 0.907440185546875, Accuracy: 0.699999988079071\n",
      "Batch number: 111, Training: Loss: 0.8558406829833984, Accuracy: 0.699999988079071\n",
      "Batch number: 112, Training: Loss: 0.9228814840316772, Accuracy: 0.699999988079071\n",
      "Batch number: 113, Training: Loss: 1.067461371421814, Accuracy: 0.5\n",
      "Batch number: 114, Training: Loss: 0.9803698658943176, Accuracy: 0.6000000238418579\n",
      "Batch number: 115, Training: Loss: 0.9028456807136536, Accuracy: 0.800000011920929\n",
      "Batch number: 116, Training: Loss: 0.8186795115470886, Accuracy: 0.699999988079071\n",
      "Batch number: 117, Training: Loss: 0.9333146810531616, Accuracy: 0.6000000238418579\n",
      "Batch number: 118, Training: Loss: 0.5350010395050049, Accuracy: 0.800000011920929\n",
      "Batch number: 119, Training: Loss: 0.7377375960350037, Accuracy: 0.6000000238418579\n",
      "Batch number: 120, Training: Loss: 1.455708384513855, Accuracy: 0.30000001192092896\n",
      "Batch number: 121, Training: Loss: 1.8942400217056274, Accuracy: 0.4000000059604645\n",
      "Batch number: 122, Training: Loss: 0.7252748012542725, Accuracy: 0.699999988079071\n",
      "Batch number: 123, Training: Loss: 0.8847925662994385, Accuracy: 1.0\n",
      "Epoch : 0, Training: Loss: f1.2921547939500864, Accuracy: 60.17842695104911%, \n",
      "\t\tValidation : Loss : 0.21255616936832666, Accuracy: 92.30769161994641%, Time: 166.69187569618225s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91628\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 0, Test: Loss: 0.049381356686353683, Accuracy: 1.0\n",
      "Test accuracy: 0.050505050505050504\n",
      "Test Batch number: 1, Test: Loss: 0.022839510813355446, Accuracy: 1.0\n",
      "Test accuracy: 0.10101010101010101\n",
      "Test Batch number: 2, Test: Loss: 0.28585997223854065, Accuracy: 0.8999999761581421\n",
      "Test accuracy: 0.14646464526051223\n",
      "Test Batch number: 3, Test: Loss: 0.04226775839924812, Accuracy: 1.0\n",
      "Test accuracy: 0.19696969576556272\n",
      "Test Batch number: 4, Test: Loss: 0.14028964936733246, Accuracy: 1.0\n",
      "Test accuracy: 0.24747474627061325\n",
      "Test Batch number: 5, Test: Loss: 0.042556945234537125, Accuracy: 1.0\n",
      "Test accuracy: 0.2979797967756637\n",
      "Test Batch number: 6, Test: Loss: 0.1610698401927948, Accuracy: 0.8999999761581421\n",
      "Test accuracy: 0.34343434102607495\n",
      "Test Batch number: 7, Test: Loss: 0.15353187918663025, Accuracy: 1.0\n",
      "Test accuracy: 0.39393939153112545\n",
      "Test Batch number: 8, Test: Loss: 0.1850665956735611, Accuracy: 0.8999999761581421\n",
      "Test accuracy: 0.4393939357815367\n",
      "Test Batch number: 9, Test: Loss: 0.23342597484588623, Accuracy: 0.8999999761581421\n",
      "Test accuracy: 0.4848484800319479\n",
      "Test Batch number: 10, Test: Loss: 0.28746774792671204, Accuracy: 0.8999999761581421\n",
      "Test accuracy: 0.5303030242823591\n",
      "Test Batch number: 11, Test: Loss: 0.12415710836648941, Accuracy: 1.0\n",
      "Test accuracy: 0.5808080747874096\n",
      "Test Batch number: 12, Test: Loss: 0.20480510592460632, Accuracy: 0.8999999761581421\n",
      "Test accuracy: 0.6262626190378209\n",
      "Test Batch number: 13, Test: Loss: 0.4801764488220215, Accuracy: 0.8999999761581421\n",
      "Test accuracy: 0.671717163288232\n",
      "Test Batch number: 14, Test: Loss: 0.39912915229797363, Accuracy: 0.8999999761581421\n",
      "Test accuracy: 0.7171717075386432\n",
      "Test Batch number: 15, Test: Loss: 0.05481913685798645, Accuracy: 1.0\n",
      "Test accuracy: 0.7676767580436937\n",
      "Test Batch number: 16, Test: Loss: 0.21864132583141327, Accuracy: 0.8999999761581421\n",
      "Test accuracy: 0.813131302294105\n",
      "Test Batch number: 17, Test: Loss: 0.2502739131450653, Accuracy: 0.8999999761581421\n",
      "Test accuracy: 0.8585858465445162\n",
      "Test Batch number: 18, Test: Loss: 0.040710002183914185, Accuracy: 1.0\n",
      "Test accuracy: 0.9090908970495667\n",
      "Test Batch number: 19, Test: Loss: 0.17750634253025055, Accuracy: 1.0\n",
      "Test accuracy: 0.9494949374536071\n",
      "{0: 'Leopards', 1: 'airplanes', 2: 'butterfly', 3: 'camera', 4: 'elephant', 5: 'lamp', 6: 'rhino', 7: 'umbrella', 8: 'watch'}\n"
     ]
    }
   ],
   "source": [
    "    # split dat\n",
    "    classes = ['Leopards', 'airplanes', 'butterfly', 'camera', 'elephant', 'lamp', 'watch', 'umbrella', 'rhino'];\n",
    "    split_data('101_ObjectCategories', './', classes)\n",
    "    \n",
    "    # transform data\n",
    "    image_transforms = input_transform()\n",
    "    \n",
    "    # created dataset\n",
    "    batch_size = 10\n",
    "    data = load_data()\n",
    "    \n",
    "    # create a data loader instance with each dataset with a batch size of 10 and shuffling\n",
    "    train_data = loader.DataLoader(data['train'], batch_size=batch_size, shuffle=True)\n",
    "    validation_data = loader.DataLoader(data['validation'], batch_size=batch_size, shuffle=True)\n",
    "    test_data = loader.DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #data size\n",
    "    train_data_size = len(data['train'])\n",
    "    validation_data_size = len(data['validation'])\n",
    "    test_data_size =  len(data['test'])\n",
    "    \n",
    "    # get model and change r50 model output to classify\n",
    "    model = def_model()\n",
    "    \n",
    "    # select optimiser and define loss func\n",
    "    loss_func, optimizer = opt_loss(model)\n",
    "    \n",
    "    # fitting / backpropagation\n",
    "    num_epochs = 30\n",
    "    trained_model, history = train_and_validate(model, loss_func, optimizer, num_epochs)\n",
    "    \n",
    "    # accuracy on testdata\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    fc_inputs = model.fc.in_features\n",
    "\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(fc_inputs, 2048),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(2048, 10),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.LogSoftmax(dim=1))\n",
    "\n",
    "    if(torch.cuda.is_available()):\n",
    "        model = model.to(\"cuda\")\n",
    "    model.load_state_dict(torch.load('model_0.pth'))\n",
    "\n",
    "    computeModelAccuracy(model, loss_func)\n",
    "    \n",
    "    # testing \n",
    "    index_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
    "    print (index_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4ca7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
